<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tinglin Huang</title>
  
  <meta name="author" content="Tinglin Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="https://netdna.bootstrapcdn.com/font-awesome/3.2.1/css/font-awesome.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:65%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tingle Li</name>
                <img src="images/Tinglin-CN.png" height="80" style="margin-bottom:-37px">
		<br>
              </p>
              <p>
                I am a first-year Ph.D. student at <a href="https://graph-and-geometric-learning.github.io/">Yale University</a>, advised by Prof. <a href="https://cs.stanford.edu/people/rexy/">Rex Ying</a>.
      		    </p>
      		    <p>
                Previously, I worked with Prof. <a href="https://keg.cs.tsinghua.edu.cn/jietang/">Jie Tang</a>, Prof. <a href="https://ericdongyx.github.io/">Yuxiao Dong</a> from Tsinghua University, and Prof. <a href="https://www.chuatatseng.com/">Tat-Seng Chua</a>, Prof. <a href="https://xiangwang1223.github.io/">Xiang Wang</a> from National University of Singapore.
              </p>
              <p>
                My research interests revolve around graph neural networks and computational biology.
              </p>
              <p>
              
               
              </p>
              <p style="text-align:center">
                <a href="mailto:huangtinglin@outlook.com"><i class="icon-envelope"></i></a> &nbsp
                <a href="files/huangtinglin_cv.pdf"><i class="ai ai-cv"></i></a> &nbsp
                <a href="https://scholar.google.com/citations?user=izW2ygYAAAAJ&hl=en"><i class="ai ai-google-scholar"></i></a> &nbsp
                <a href="https://github.com/huangtinglin"><i class="icon-github"></i></a>
    
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/avatar.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/avatar.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
		2022
                  <ul>
		      <li>One paper was accepted to <a href="https://eccv2022.ecva.net/">ECCV 2022</a>.</li>
                      <li>One paper was accepted to <a href="https://interspeech2022.org/">Interspeech 2022</a>.</li>
                  </ul>

                2021
                  <ul>
                      <li>One paper was accepted to <a href="https://nips.cc/Conferences/2021/">NeurIPS 2021</a>.</li>
                      <li>One paper was accepted to <a href="https://www.interspeech2021.org/">Interspeech 2021</a> with <a href="https://www.interspeech2021.org/student-information/travel-grants">ISCA Travel Grant Award</a>.</li>
                  </ul>
              </p>
            </td>
          </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <heading>Publications</heading>
        

          </td>
            </tr>
        </tbody></table>
	

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
          <td width="40%">
            <img src="images/LFT-teaser.gif" alt="lft" height="160" style="border-style: none">
          </td>
              <td width="75%" valign="middle">
                <a href="https://tinglok.netlify.app/files/avstyle/resources/preprint.pdf">
                  <papertitle>Learning Visual Styles from Audio-Visual Associations</papertitle>
                </a>
                <br>
                <b>Tingle Li</b>, <a href="https://www.linkedin.com/in/yichen-liu-751804176/">Yichen Liu</a>, <a href="https://andrewowens.com/">Andrew Owens</a>, <a href="https://hangzhaomit.github.io/">Hang Zhao</a>
                <br>
                <em>ECCV</em>, 2022 &nbsp 
                <br>
                <a href="https://arxiv.org/pdf/2205.05072.pdf">PDF</a> / <a href="https://tinglok.netlify.app/files/avstyle/">Project Page</a> / <a href="https://github.com/Tinglok/avstyle"> Code </a> 
                <p>We learn from unlabeled data to manipulate the style of an image using sound.</p>
              </td>
          </tr>
		
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
          <td width="40%">
            <img src="images/R2S-teaser.jpg" alt="r2s" height="185" style="border-style: none">
          </td>
              <td width="75%" valign="middle">
                <a href="https://tinglok.netlify.app/files/avstyle/resources/preprint.pdf">
                  <papertitle>Radio2Speech: High Quality Speech Recovery from Radio Frequency Signals</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=QDnm0fgAAAAJ&hl=en">Running Zhao</a>,Â <a href="http://www.jiangtaoyu.com/">Jiangtao Yu</a>, <b>Tingle Li</b>, <a href="https://hangzhaomit.github.io/">Hang Zhao</a>, <a href="https://www.eee.hku.hk/people/echngai/">Edith C.H. Ngai</a>
                <br>
                <em>Interspeech</em>, 2022 &nbsp 
                <br>
                <a href="https://arxiv.org/pdf/2206.11066.pdf">PDF</a> / <a href="https://zhaorunning.github.io/Radio2Speech/">Project Page</a>
                <p>We use RF signals to recover high quality speech.</p>
              </td>
          </tr>
		
          <tr>
          <td width="40%">
            <img src="images/UMT-teaser.jpg" alt="umt" height="80" style="border-style: none">
          </td>
              <td width="75%" valign="middle">
                <a href="https://openreview.net/pdf?id=1eGFH6yYAJn">
                  <papertitle> Modality Laziness: Everybody's Business is Nobody's Business </papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=VoF-UAEAAAAJ&hl=en">Chenzhuang Du</a>, <a href="https://www.tengjiaye.com/">Jiaye Teng</a>, <b>Tingle Li</b>, <a href="https://www.linkedin.com/in/yichen-liu-751804176/">Yichen Liu</a>, <a href="https://people.csail.mit.edu/yuewang/">Yue Wang</a>, <a href="http://people.iiis.tsinghua.edu.cn/~yuanyang/en.html">Yang Yuan</a>, <a href="https://hangzhaomit.github.io/">Hang Zhao</a>
                <br>
                <em>arXiv</em>, 2022 &nbsp 
                <br>
                <a href="https://openreview.net/pdf?id=1eGFH6yYAJn">PDF</a>
                <p></p>
                <p>With multi-modal data as inputs, the encoders from naive fusion training will suffer from learning insufficient representations of each modality. </p>
              </td>
          </tr>
		
		
	  <tr>
            <td width="40%">
              <img src="images/AVD-teaser.png" alt="avd" height="142" style="border-style: none">
            </td>
                <td width="75%" valign="middle">
                  <a href="https://papers.nips.cc/paper/2021/file/8a9c8ac001d3ef9e4ce39b1177295e03-Paper.pdf">
                    <papertitle> Neural Dubber: Dubbing for Videos According to Scripts </papertitle>
                  </a>
                  <br>
                  <a href="https://huchenxucs.github.io/">Chenxu Hu</a>, <a href="https://scholar.google.com/citations?user=PMH1tnEAAAAJ&hl=en">Qiao Tian</a>, <b>Tingle Li</b>, Yuping Wang, <a href="https://www.linkedin.com/in/yuxuan-wang-85b5704">Yuxuan Wang</a>, <a href="https://hangzhaomit.github.io/">Hang Zhao</a>
                  <br>
                  <em>NeurIPS</em>, 2021 &nbsp 
                  <br>
                  <a href="https://papers.nips.cc/paper/2021/file/8a9c8ac001d3ef9e4ce39b1177295e03-Paper.pdf">PDF</a> / <a href="https://tsinghua-mars-lab.github.io/NeuralDubber/">Project Page</a> / <a href="https://slator.com/neural-dubber-tiktok-company-bytedance-explores-automated-dubbing/">Press</a>
                  <p></p>
                  <p>Automatic video dubbing driven by a neural network. </p>
                </td>
            </tr>

		        <tr>
            <td width="40%">
              <img src="images/CVC-teaser.jpg" alt="CVC" height="132" style="border-style: none">
            </td>
                <td width="75%" valign="middle">
                  <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/li21d_interspeech.pdf">
                    <papertitle> CVC: Contrastive Learning for Non-parallel Voice Conversion </papertitle>
                  </a>
                  <br>
                  <b>Tingle Li</b>, <a href="https://www.linkedin.com/in/yichen-liu-751804176/">Yichen Liu</a>, <a href="https://huchenxucs.github.io/">Chenxu Hu</a>, <a href="https://hangzhaomit.github.io/">Hang Zhao</a>
                  <br>
                  <em>Interspeech</em>, 2021 <font color="red"><strong>(ISCA Student Travel Grant)</strong></font>&nbsp 
                  <br>
                  <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/li21d_interspeech.pdf">PDF</a> / <a href="https://tinglok.netlify.app/files/cvc"> Project Page</a> / <a href="https://github.com/Tinglok/CVC"> Code </a> 
                  <p></p>
                  <p> One-way GAN training for non-parallel voice conversion. </p>
                </td>
              </tr>
		
            <tr>
            <td width="40%">
              <img src="images/Sams-teaser.jpg" alt="sams" height="202" style="border-style: none">
            </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/1909.05746.pdf">
                    <papertitle> Sams-Net: A Sliced Attention-based Neural Network for Music Source Separation </papertitle>
                  </a>
                  <br>
                  <b>Tingle Li</b>, Jiawei Chen, Haowen Hou, <a href="https://scholars.duke.edu/person/MingLi">Ming Li</a>
                  <br>
                  <em>ISCSLP</em>, 2021 <font color="red"><strong>(Oral, Best Undergraduate Dissertation)</strong></font>&nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/1909.05746.pdf">PDF</a> / <a href="https://tinglok.netlify.app/files/samsnet/">Project Page</a> 
                  <p></p>
                  <p> The scope of attention is narrowed down to the intra-chunk musical features that are most likely to affect each other. </p>
                </td>
              </tr>
		
          	<tr>
            <td width="40%">
              <img src="images/FSC-teaser.jpg" alt="fsc" height="100" style="border-style: none">
            </td>
                <td width="75%" valign="middle">
                  <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2020/lin20e_interspeech.pdf">
                    <papertitle> The DKU Speech Activity Detection and Speaker Identification Systems for Fearless Steps Challenge Phase-02 </papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=GM8wpNcAAAAJ&hl=en">Qingjian Lin</a>, <b>Tingle Li</b>, <a href="https://scholars.duke.edu/person/MingLi">Ming Li</a>
                  <br>
                  <em>Interspeech</em>, 2020 &nbsp 
                  <br>
                  <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2020/lin20e_interspeech.pdf">PDF</a> / <a href="https://fearless-steps.github.io/ChallengePhase2/Final.html">Leaderboard</a>
                  <p></p>
                  <p>SoTA performance for speech activity detection and speaker identification. </p>
                </td>
            </tr>
		
          	<tr>
              <td width="40%">
                  <img src="images/Atss-teaser.jpg" alt="atss" height="185" style="border-style: none">
              </td>
                <td width="75%" valign="middle">
                  <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2020/li20p_interspeech.pdf">
                    <papertitle> Atss-Net: Target Speaker Separation via Attention-based Neural Network </papertitle>
                  </a>
                  <br>
                  <b>Tingle Li</b>, <a href="https://scholar.google.com/citations?user=GM8wpNcAAAAJ&hl=en">Qingjian Lin</a>, Yuanyuan Bao, <a href="https://scholars.duke.edu/person/MingLi">Ming Li</a>
                  <br>
                  <em>Interspeech</em>, 2020 &nbsp 
                  <br>
                  <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2020/li20p_interspeech.pdf">PDF</a> / <a href="https://tinglok.netlify.app/files/interspeech2020/">Project Page</a>
                  <p></p>
                  <p>Adapted Transformer to the speech separation for more efficient and generalizable performance. 
              </tr>
		
		
        		<tr>
              <td width="40%">
                  <img src="images/OML-teaser.jpg" alt="oml" height="156" style="border-style: none">
              </td>
                <td width="75%" valign="middle">
                  <a href="https://www.isca-speech.org/archive/pdfs/odyssey_2020/lin20_odyssey.pdf">
                    <papertitle> Optimal Mapping Loss: A Faster Loss for End-to-End Speaker Diarization </papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=GM8wpNcAAAAJ&hl=en">Qingjian Lin</a>, <b>Tingle Li</b>, Lin Yang, Junjie Wang, <a href="https://scholars.duke.edu/person/MingLi">Ming Li</a>
                  <br>
                  <em>Odyssey</em>, 2020 &nbsp 
                  <br>
                  <a href="https://www.isca-speech.org/archive/pdfs/odyssey_2020/lin20_odyssey.pdf">PDF</a>
                  <p></p>
                  <p>A new mapping loss based on Hungarian algorithm that reduces time complexity while maintaining performance for speaker diarization.
              </tr>
    

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right">
                <!-- <font size="2"> -->
                  Last updated Aug. 2022.
                <br>
                <!-- <font size="2"> -->
                  Template from <a href="https://jonbarron.info/">Jon Barron</a>.
                <!-- </font> -->
              </p>

        </table>

        <script type="text/javascript">
        var sc_project=12691142; 
        var sc_invisible=1; 
        var sc_security="b5064dc1"; 
        </script>
        <script type="text/javascript"
        src="https://www.statcounter.com/counter/counter.js"
        async></script>
        <noscript><div class="statcounter"><a title="Web Analytics"
        href="https://statcounter.com/" target="_blank"><img
        class="statcounter"
        src="https://c.statcounter.com/12691142/0/b5064dc1/1/"
        alt="Web Analytics"
        referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>



        
</body>
</html>
